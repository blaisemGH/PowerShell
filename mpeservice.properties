# the log path of the MPEService is the path where the MPE log files are put to a subdirectory log_Job_xxx
mpeservice.logPath = /opt/abacus/log/mpeservice

# the file path is used as a temporary directory for the file upload and temporary files for the SQL*Loader
mpeservice.filePath = /opt/abacus/temp/

# the input path is the path where the uploaded files are stored to a subdirectory defined in the table davinci_user (attribute path) in the master database schema
mpeservice.inputPath = /opt/abacus/input

# common log folder, optional
abacus360.logs.directory = /opt/abacus/log

# the user of the edit database schema
mpe.editSchema = abacus_edit

# the user of the temporary database schema; more database schemas can be defined [space] separated if MPE should execute several parallel processings of calculation jobs
mpe.tempSchemas = abacus_temp01 abacus_temp02 abacus_temp03 abacus_temp04 abacus_temp05

# is used to specify a group of eligible temp schemas where an MPE process should run
# more groups can be defined as: mpe.schemagroup.<name of group>
# more database schemas can be defined [space] separated if MPE should execute several parallel processings of calculation jobs
# add --schemagroup=group1 to an MPE process to run it only on the schemas in group1
# the special group 'webserver' (configured by mpe.tempSchemaGroup.webserver) will be used as the schema group for all processes started via the application GUI
#mpe.tempSchemaGroup.group1 = abacus_temp

# the archived periods can be automatically deleted if the archiving has been finished successfully (1 = delete, 0 = do not delete / archive only)
mpeservice.deleteAfterArchive = 0

# copy the file (on system level) in smaller pieces
# (this sizing parameter is optional: the files will be copied in the whole if the the parameter is not defined)
# currently only used in mpeDataExport (Archivierung)
#mpeservice.copyChunkSize=67000000

# send Notification, only if process has error (default YES , meaning to send always)
#mpeservice.notify=ERROR

# used only in Engine-1.4.0.00
# same usage as mpeservice.notify
# when both present, mpeservice.notify has priority
#mpeService.notify=ERROR

# set lock edit partitions on true if all edit partitions should automatically be locked during a running mpe process
mpeservice.lockEditPartitions = false

# assumes no double primary keys in the interface files (C200, p019, ... Unique)
# if false: the data is merged per primary key
# if true: data is loaded one by one
# default: true
#jcal.ModeUnique=true

# set to yes to switch adaptive statistics calculation in TEMP schema on
# Adpative statistics calculation will try to limit the amount of statistics calculations that occur on
# temp schema tables, by keeping the last statistics. Only if the volume of data changes significantly 
# (if the volume double compared to the last time the statistics were built), the statistics will be recalculated. 
mpe.adaptiveStatistics = no

# set to the number of times a failed query will be executed again
mpe.queryretry = 1

# set to time (in ms) the system will wait in order to retry a query
mpe.queryretrytime = 1000

# Use DML Statements in parallel (force parallel dml)
mpe.parallelDML = true

# Use DDL Statements in parallel (force parallel ddl)
mpe.parallelDDL = true

# force parallel (if mpe.parallelDML resp. mpe.parallelDDL is true) 
mpe.forceParallel = false

# set parallel degree for indexes, default: parallelDegree
mpe.parallelDegreeIndex = 

# set parallel degree for temporary tables which are created during the batch process, default: parallelDegree
# A parallel degree for the table of a certain MPE model can also be defined by mpe.parallelDegreeTable.<modelName>,
# example: mpe.parallelDegreeTable.CreateRT30b_normal=2
mpe.parallelDegreeTable = 

# set parallel degree for models, default: parallelDegree
# A parallel degree for the threads of a certain MPE model can also be defined by mpe.parallelDegreeModels.<modelName>,
# example: mpe.parallelDegreeModels.CreateRT30b_normal=2
mpe.parallelDegreeModels = 

# use alter session set sql_trace=true before all executions of MPEPreparedStatements, MPEStatements and query elements, default: false
# if true:
# - all MPEConnections are initialized with: alter session set sql_trace=false
# - only for query elements alter session set sql_trace=false is executed at the end of the query
# - for MPEPreparedStatements and MPEStatements alter session set sql_trace=false is not executed to avoid a reset of the existing ResultSet (error java.sql.SQLException: Stream wurde schon geschlossen in XMLLoader)
# requirements for true: connect with system user and execute the following for all temporary schemas that are active: grant alter session to <abacus_temporary_schema>
# please set this parameter to true only for performance analysis of some MPE processes as the Oracle trace files will be very big
# the Oracle trace files can be found in the Oracle udump directory and can be evaluated by the Oracle tool tkprof <trace_file>.trc <trace_file>.txt
# the information in <trace_file>.txt can be useful to to check the optimizer execution plans for all executed queries at the end of the MPE process
mpe.sql_trace = false

# mpe.optimizeWithUnusableIndexesAndIndexRebuild (default true) defines the behaviour of the "optimize" feature in MPE processes:
# - true: set indexes to unusable in the beginning, rebuild them in the end 
# - false: drop indexes in the beginning, create them in the end
# Unique indexes will always be handled with the drop/create pattern.
mpe.optimizeWithUnusableIndexesAndIndexRebuild = true

# used to remove the hints from the queries before all executions of SQL statements in MPE processes
# default: false
# mpe.ignoreSqlHints=true

# mpe.usePartitionedModels (default true) defines if the MPE transformation models should create the temporary input tables as hash-partitioned tables. 
mpe.usePartitionedModels = true

# Hostname of the proxy server
# http.proxyHost=

# Port on which the proxy server listens for incoming requests
# http.proxyPort=

# The username to be sent to the proxy
# http.proxyUser=

# The password to be sent to the proxy.
# http.proxyPassword=

#Compression is not available in Oracle Standard Edition and should be disabled in this case. It is however available in Oracle Enterprise Edition. 
#If parameter not defined it defaults to yes. 
#Accepted values: yes/no. In case of other values, default value is taken.  
mpe.archiveCompression = yes
# The clustername, given in many of the mpe-jobs as an parameter, can be validated by setting a regular expression. The validation is made in the mpe-service before invoking the job. 
# The default, if nothing is configured is: [\w\_\-\.]*?
# mpe.clustername_regex=

# Enables the parallel creation of XML files for the SQLtoXML task.
# If it is not set, SQLtoXML will not generate parallel XML files.
# If it is set, the system will do the following:
# * If set to 1, no parallel generation will take place
# * If set to > 1, it defines the number of parallel files will be generated
# * If set to < 1, the normal system defined "parallelDegree" number of files will be generated. 
# For a system with a larger number of cores, this setting enhances the performance of the XML file creation
# mpe.parallelDegreeSQLtoXML=1

# Parallelization degree for anonymization steps - defaults to parallelDegree.
# mpe.parallelDegreeXMLAnonymizer=1

# Sets the threshold of the number of records at which the parallel creation of XML files for SQLtoXML task is enabled.
# For small tables it is normally not very advantageous to create multiple files in parallel.
# mpe.parallelThresholdSQLtoXML=100000

#Set concatenation option for the export tables process
#May increase performance for large datasets
SQLtoXML.useConcatenate = false

#Set the value of the separator to be used in case of concatenation
#Default value is ^
SQLtoXML.concatenateSeparator = 
#Specify to use TO_CLOB in case of concatenation, to avoid ORA-01489
SQLtoXML.useClob = false

# is used to check if the files send as parameters in mpeprocesses have the same extension as the files allowed to be uploaded 
# if true: all the files  are checked 
# default: false
mpe.allowOnlyGUIFileTypes = false

# set the encoding for exported csv-files (default ISO-8859-15), xml files are always exported in UTF-8
# csvFileEncoding = UTF-8

# is used to activate or deactivate the check of the dependencies for the tasks associated with the dashboard
# true - the dependencies for the tasks will be checked
# false - the dependencies for the tasks will not be checked
# default: false
# mpe.reportingMonitorDependenciesCheck = false

# sets the retry intervall used by MPEBatch when starting a job without --queue
mpe.batch.retry.interval = 10000
# sets the variance for the retry ( mpe.batch.retry.interval + (Math.round(((Math.random()*mpe.batch.retry.interval*mpe.batch.retry.variance))))
# e.g. with interval 15 and variance 3 the interval will be between 15 and 60 seconds
mpe.batch.retry.variance = 0

# With mpe.queueTriggers, space separated names of classes can be given that shall execute when a process is added to the queue
# Classes must implement com.bearingpoint.davinci.mpeservice.ProcessQueueTrigger
# mpe.queueTriggers=

# With mpe.startTriggers, space separated names of classes can be given that shall execute at the start of each process.
# Classes must implement com.bearingpoint.davinci.mpeservice.ProcessStartTrigger
# mpe.startTriggers=

# With mpe.finishTriggers, space separated names of classes can be given that shall execute at the end of each process (successful or cancelled or failed)
# Classes must implement com.bearingpoint.davinci.mpeservice.ProcessFinishtTrigger
# mpe.finishTriggers=

# Sets whether the MPE process should stop (fail with error) if there are any exceptions in the execution of the process triggers (start triggers or finish triggers)
# true - if there is an exception in the trigger, the MPE process will stop with errors
# false - if there is an exception in the trigger, the MPE process will continue and the exception will be logged in the application log
# Default value is true
# mpe.stopProcessOnTriggerException = true

# Sets whether the reporting monitor related process triggers should search for all the matches for the current MPE process or only for the first one.
# true - the process triggers will check only for one match for the current MPE process.
# false - the process triggers will check for all the matches for the current MPE process.
# Default value is false.
#mpe.stopAfterFirstMatchInProcessTrigger = false

# Space separated (metaMPE) names of the processes for which the process triggers should always search for all the occurrences
# (thus ignoring if the mpe.stopAfterFirstMatchInProcessTrigger property is set to true).
#mpe.processesToSearchForAllMatchesInProcessTrigger = 

# Sets whether the reporting monitor related process triggers will check all the entities from the reporting monitor for matches or only the entity of the MPE process. 
# true - the process triggers will check only the entity of the MPE process.
# false - the process triggers will check all the entities defined in the reporting monitor.
# Default value is false.
#mpe.checkOnlyProcessEntityInProcessTrigger = false

# base name for logfile of batch jobs (i.e. MPE jobs)
#mpe.logFileName=process


# Dedicated parallel degree for pivoted tables (default parallel degree)
# this parallel degree can be defined for better performance (higher parallelization and PGA usage) of SQL queries which are related to the RT930 verticalization
# (necessary to enable optional parallel degree parameter for recs_type_930_base, recs_type_930_v and recs_type_930_calc and tables)
# this parallel degree is used e.g. in the Java Native Classes CreatePivotTables and UnpivotTables and in the MERGE INTO queries for table recs_type_930_v in process step loadercorrection
# mpe.parallelDegreePivotTables=

# Force parallel degree for pivot table operations (default false).
# When mpe.parallelForcePivotTables=true is set then maximum parallelism is used for the DDL and DML statements in the Java Native Classes CreatePivotTables and UnpivotTables
# This will use as much available CPU ressources of Oracle DB Server as possible with highest PGA memory usage and in optimal case no TEMP tablespace.
# If mpe.parallelForcePivotTables is activated then an "alter session force parallel dml" or "alter session force parallel ddl" statement is executed
# and the number of Oracle parallel threads can even be higher than specified with the parallel hints in the Java Native Classes CreatePivotTables and UnpivotTables.
# mpe.parallelForcePivotTables=true

# Sets the percentage for gathering table statistics with meta_ddl.analyze_table (for MPE element or attribute optimize) after recreating or rebuilding the indexes for a table
# NULL - for gathering table statistics with DBMS_STATS.AUTO_SAMPLE_SIZE
# Default: NULL
# mpe.gatherTableStatsPercentage=1

# Maximum size (in bytes) of temporary files created in process steps that are similar to downloading. Default value (1073741824) represents 1 Gigabyte.
# mpe.download.tempFiles.maxSize = 1073741824

# Default decimal format for the <report> feature in MPE
# mpe.genericReporter.defaultDecimalFormat = ###0.00

# Default Date format for the <report> feature in MPE
# mpe.genericReporter.defaultDateFormat = yyyy-MM-dd

# Default DateTime format for the <report> feature in MPE
# mpe.genericReporter.defaultDateTimeFormat = yyyy-MM-dd HH:mm:ss

# Use parallelism for creating indexes during MPE processes (with additional "alter session enable parallel ddl" and "alter session disable parallel ddl")
# Default: false 
# mpe.parallelDDLCreateIndex=true

# Max Number of parallel issued DDL index creation commands 
# Default: 1
# mpe.parallelDegreeIndexThreads=3

# Count of output records to be collected in global temporary table in MPE Transformation execution phase before writing it to the final output table.
# The defined value should be higher than value of parameter ModelCommitEvery (default: 1000) in jcal.properties.
# After the transfer of the output records a commit is done. 
# Remark: If mpe.useDirectWritingToOutputTable=true is set then no transfer to the final output table is necessary as the records were already inserted into the final output table before but the commit is done.
# default: 100000
mpe.transferThreshold = 100000

# EXPERIMENTAL FEATURE! If enabled (mpe.createIndexesUnusable=true), the indexes will be created in state unusable and after the creation be build in one step.
# If disabled, the indexes will be created like any ordinary index. Default values is false.
# mpe.createIndexesUnusable=false

# Controls if a failed process can be continued. Defaults to true. Setting to false reduces file system accesses to the "restart" folder.
# mpe.enableContinue=false

# mpe.xlsImportDecimalSeparator is used to configure the locale for converting the numeric values in a XLS(X) file which is imported. It can take the following values:
# . to use the Locale.US. For example: 123,123.22
# , to use the Locale.GERMAN. For example: 123.123,22
# If xlsImportDecimalSeparator is not defined then the locale will be determined, if possible, based the format of the first numeric cell to be imported.
# If the locale could not be determined by inspecting the first numeric cell, then Locale.GERMAN will be used to convert the numbers in the XLS(X) file.
# A numeric cell is considered to be a cell which has to be imported in a database column of type DOUBLE, NUMBER or FLOAT.
# mpe.xlsImportDecimalSeparator=.

# MPE names to handle their calculated data as delivered. Typically these are the converter processes between DaVinci and Abacus360.
# Default: mpeConvertDataModel,mpeConvertDataModel2
# mpe.deliveringProcesses=mpeConvertDataModel,mpeConvertDataModel2


# Specify the WildFly nodes for MPE processing, this parameter controls if the MPEServiceImpl will check the JobManager queue, in a schedule, so that waiting jobs are started after running jobs finishes.
# If it's set to false the scheduled check won't happen (should be used only on clusters/nodes that should not handle <execute> MPE processes).
# Optional: If no parameter is set, MPE processing can run on all Application Servers.
# The node name is composed from the hostname, "-" character and the given node name. (for example: localhost-node1, localhost-node2, localhost-node3)
# mpe.checkQueueEnabled.<hostname>-<node1>=false
# mpe.checkQueueEnabled.<hostname>-<node2>=true


#
# For explanation of the following CSV loader properties, first a short overview of how it works (as delivered with Abacus Engine R2.3.0.00):
# Every input file (CSV or ZIP) is read by a separate Java thread (CSV producer). The data are bundled into packages and these packages are
# placed on a queue (data package queue). These data packages still contain only text.
# Other Java threads (CSV consumers) take the packages from the data package queue and create Entity objects out of the contained data. These
# created Entities are then again bundled into packages of the same size and placed on another queue (writer queue).
# Some writer threads take these Entity packages from the writer queue and save them into memory and database (temp schema).
#

# Package size
# Default: 1000
# mpe.csvloader.packagesize=1000

# Number of CSV consumers
# Default: 5
# mpe.csvloader.csvconsumer.count=5

# Time in milliseconds for the CSV consumers to wait, when no package is available on the data package queue, before checking again
# Default: 10
# mpe.csvloader.csvconsumer.polling.waitms=10

# Time in milliseconds for the CSV consumer to wait, when the writer queue is full, before checking again
# Default: 10
# mpe.csvloader.csvconsumer.offering.waitms=10

# Time in milliseconds for the CSV producer to wait, when the data package queue is full, before checking again
# Default: 10
# mpe.csvloader.csvproducer.offering.waitms=10

# Length of the data package queue
# Default: 20
# mpe.csvloader.datapackage.queuelength=20

# Number of writers
# Maximum: 26
# Default: 10
# mpe.csvloader.writer.count=10

# Length of the writer queue
# Default: 20
# mpe.csvloader.writer.queuelength=20


# Temporary workaround for timeouts during grouping. Time unit hours
# Default: 3
mpe.maxGroupingTimeout = 3

# Configurable timeout for worker response time in milliseconds.
# Default: 600000 (10 minutes)
mpe.waitForStatusAnswer = 600000

# Configurable interval for checking workernode status
# Default: 90000 (90 seconds)
mpe.waitTimeIntervall = 90000

# Declares which processes make long running calls to the server. Such processes are configured with a bigger invocation timeout in order for the call to the server to finish successfully.
# These timeouts can be individually configured as well in jee.properties.
# The timeouts used by processes are:
#   - processes in this list => jee.remote.client.invocation.longTimeout
#   - other processes => jee.remote.client.invocation.timeout
#
# mpe.longRequestProcesses=mpeDeleteEmptyLogicalPartitions,mpeAWV_DataReorganization,mpeAWVDeletePartitions,mpeDeletePrivateEditPartitions,mpeZipDataExport,mpeDataExport,mpeReorganizeAndDelete,mpeDeletePartitions,mpeCreateEditPartitions,mpeLockEditPartitions,mpeUnLockEditPartitions,mpeRemoveDeletedPartitions,mpeNewArchivExport,mpeNewArchivImport,mpeMergePartitions,mpeCopyPartitions,mpeCreateClusterFromBase

# Property used to configure the maximum retry count when establishing a connection to the Bundesbank in order perform the XMW/XBR validation.
# Default value: 3
# xmlparser.validate.retry.count=3
 
# Property used to configure the wait time between retry when establishing a connection to the Bundesbank in order perform the XMW/XBR validation.
# Default value: 20000
# xmlparser.validate.retry.wait=20000

# Use Oracle PQ_DISTRIBUTE(<MPE_transformation_name>+"_tmp", PARTITION) hint for performance improvement during creation of *_tmp table in MPE transformation preparation phase
# true - for using PQ_DISTRIBUTE hint (prerequisite: mpe.usePartitionedModels=true)
# Remarks:
# "parallel ddl" and "parallel query" are enabled for the session in which this query is executed for avoiding serial execution and performance improvement of run-time.
# If an Oracle error occurs during execution of this query then there is a "warn" message in the process log file and the query is executed again without PQ_DISTRIBUTE hint and with disabled "parallel ddl" and "parallel query".
# If the query fails again during the retry phase the process will be stopped and an "error" message with the Oracle error can be found in the process log file for further analysis.
# Default: false
#mpe.usePQ_DISTRIBUTEHint=false

# Use direct writing to output table without using global temporary table for performance improvement during MPE transformation execution phase.
# The insert statement is executed without Oracle append hint and with an additional random number in the comment of the SQL query to avoid Oracle TM contention and Oracle Shared Pool contention.
# true - for using direct writing instead of intermediate writing to global temporary table with transfering the records into output table with insert append hint after mpe.transferThreshold records
# Default: false
#mpe.useDirectWritingToOutputTable=false

# In case direct writing to output table for MPE transformation execution phase without using global temporary table is switched on via useDirectWritingToOutputTable=true and
# target table like recs_type_930_calc has bitmap indices with high parallel degree it could cause "enq: TX - row lock contention".
# In this case disable the direct writing to output table with useDirectWritingToOutputTable=false and set mpe.transformationAvoidContentionWhenUsingTempTable to true.
# Unique SQL IDs will be created to avoid shared pool contention (cursor: mutex S)
# Removing append hint in insert into target table will remove high "enq: TM - contention" with higher parallel degree.
# The temporary table t0#<output_table> which is used for insert query is copied finally to the output table which helps avoiding the Oracle error
# "ORA-28604: Tabelle zu fragmentiert, um Bitmap-Index (...) zu generieren" in case of later recreation of the bitmap indexes by an <optimize> element,
# see Oracle MetaLink ORA-28604 Error Occurs When Creating a Bitmap Index on a Table (Doc ID 119674.1)
# Default: false 
#mpe.transformationAvoidContentionWhenUsingTempTable=true

# The following Oracle optimizer hint properties are used in various product specific processes.
# default value for all is null (empty). No hint is set then.
# The following parameters are used on session level only for the relevant queries and should be activated for versions between Oracle 12.2 and Oracle 19.10..
#oracle12r2.optimizer_squ_bottomup=OPT_PARAM('_optimizer_squ_bottomup','FALSE')
#oracle12r2.complex_view_merging=OPT_PARAM('_complex_view_merging','FALSE')
#oracle12r2.px_wif_extend_distribution_keys=OPT_PARAM('_px_wif_extend_distribution_keys','FALSE')
# The following parameters are used on session level only for the relevant queries and should be activated for versions between Oracle 19.3 and Oracle 19.10.
#oracle19r3.px_wif_dfo_declumping=OPT_PARAM('_px_wif_dfo_declumping','off')
oracle19r3.px_wif_dfo_declumping = OPT_PARAM('_px_wif_dfo_declumping','off')


# Defer init_database to be executed at a slightly later stage before the actual process starts, allowing parallelism.
# If false, init_database is called centrally and synchronizedly by the JobManager before starting.
# Default: true
# mpeservice.deferInitDatabase=false

# An additional optional parameter in mpeservice.properties is mpe.doNotCreateBitmapIndexes. The default is 'false'. The parameter defines the
# behavior of the 'optimize' feature in MPE processes. If set to 'true', regular and bitmap indexes are dropped in the beginning, and only regular
# indexes are created in the end. Also, the bitmap indexes might be deactivated in general (depending on the available database resources and data
# volume to be processed). In case of full table access, the performance is much better compared to bitmap index access.
# This only works when parameter mpe.optimizeWithUnusableIndexesAndIndexRebuild is set to 'false'.
mpe.doNotCreateBitmapIndexes = false
